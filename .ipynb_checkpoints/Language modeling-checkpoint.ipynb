{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from math import log\n",
    "import sys\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import feature_extraction\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 3 backwards compatibility tricks\n",
    "if sys.version_info.major > 2:\n",
    "\n",
    "    def xrange(*args, **kwargs):\n",
    "        return iter(range(*args, **kwargs))\n",
    "\n",
    "    def unicode(*args, **kwargs):\n",
    "        return str(*args, **kwargs)\n",
    "\n",
    "class LangModel:\n",
    "    def fit_corpus(self, corpus):\n",
    "        \"\"\"Learn the language model for the whole corpus.\n",
    "\n",
    "        The corpus consists of a list of sentences.\"\"\"\n",
    "        for s in corpus:\n",
    "            self.fit_sentence(s)\n",
    "        self.norm()\n",
    "\n",
    "    def perplexity(self, corpus):\n",
    "        \"\"\"Computes the perplexity of the corpus by the model.\n",
    "\n",
    "        Assumes the model uses an EOS symbol at the end of each sentence.\n",
    "        \"\"\"\n",
    "        return pow(2.0, self.entropy(corpus))\n",
    "\n",
    "    def entropy(self, corpus):\n",
    "        num_words = 0.0\n",
    "        sum_logprob = 0.0\n",
    "        for s in corpus:\n",
    "            num_words += len(s) + 1 # for Linespace\n",
    "            sum_logprob += self.logprob_sentence(['_LINESPACE']+s)\n",
    "        return -(1.0/num_words)*(sum_logprob)\n",
    "\n",
    "    def logprob_sentence(self, sentence):\n",
    "        p = 0.0\n",
    "        for i in range(1,len(sentence)):\n",
    "            p += self.cond_logprob(sentence[i], sentence[:i])\n",
    "        p += self.cond_logprob('_LINESPACE', sentence)\n",
    "        return p\n",
    "\n",
    "    # required, update the model when a sentence is observed\n",
    "    def fit_sentence(self, sentence): pass\n",
    "    # optional, if there are any post-training steps (such as normalizing probabilities)\n",
    "    def norm(self): pass\n",
    "    # required, return the log2 of the conditional prob of word, given previous words\n",
    "    def cond_logprob(self, word, previous): pass\n",
    "    # required, the list of words the language model supports (including EOS)\n",
    "    def vocab(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001):\n",
    "        self.model = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        \n",
    "    def inc_word(self, w):\n",
    "        if w in self.model:\n",
    "            self.model[w] += 1.0\n",
    "        else:\n",
    "            self.model[w] = 1.0\n",
    "\n",
    "    def fit_sentence(self, sentence):\n",
    "        for w in sentence:\n",
    "            self.inc_word(w)\n",
    "        self.inc_word('_LINESPACE')\n",
    "\n",
    "    def norm(self):\n",
    "        \"\"\"Normalize and convert to log2-probs.\"\"\"\n",
    "        tot = 0.0\n",
    "        for word in self.model:\n",
    "            tot += self.model[word]\n",
    "        ltot = log(tot, 2)\n",
    "        for word in self.model:\n",
    "            self.model[word] = log(self.model[word], 2) - ltot\n",
    "\n",
    "    def cond_logprob(self, word, previous):\n",
    "        if word in self.model:\n",
    "            return self.model[word]\n",
    "        else:\n",
    "            return self.lbackoff\n",
    "\n",
    "    def vocab(self):\n",
    "        return self.model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001):\n",
    "        self.model = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        self.vocabulary = dict()\n",
    "        \n",
    "    def inc_word(self, w):\n",
    "        if w in self.model:\n",
    "            self.model[w] += 1.0\n",
    "        else:\n",
    "            self.model[w] = 1.0\n",
    "\n",
    "    def fit_sentence(self, sentence):\n",
    "        s = ['_LINESPACE'] + sentence + ['_LINESPACE']\n",
    "        for w in s:\n",
    "            self.inc_word(w)\n",
    "            self.vocabulary[w] = 1\n",
    "        #self.inc_word('_LINESPACE')\n",
    "        \n",
    "        for i in range(2,3):\n",
    "            for j in range(len(s)-i+1):\n",
    "                self.inc_word(tuple(s[j:j+i]))\n",
    "\n",
    "    def cond_logprob(self, word, previous):\n",
    "        if (len(previous)>=1 and tuple([previous[-1]] + [word]) in self.model):\n",
    "            return log(self.model[tuple([previous[-1]] + [word])]/self.model[previous[-1]],2)\n",
    "        elif word in self.model:\n",
    "            return log(self.model[word]/len(self.vocabulary),2)\n",
    "        else:\n",
    "            return self.lbackoff\n",
    "\n",
    "    def vocab(self):\n",
    "        return self.vocabulary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trigram(LangModel):\n",
    "    def __init__(self, backoff = 0.000001):\n",
    "        self.model = dict()\n",
    "        self.lbackoff = log(backoff, 2)\n",
    "        self.vocabulary = dict()\n",
    "        \n",
    "    def inc_word(self, w):\n",
    "        if w in self.model:\n",
    "            self.model[w] += 1.0\n",
    "        else:\n",
    "            self.model[w] = 1.0\n",
    "\n",
    "    def fit_sentence(self, sentence):\n",
    "        s = ['_LINESPACE'] + sentence + ['_LINESPACE']\n",
    "        for w in s:\n",
    "            self.inc_word(w)\n",
    "            self.vocabulary[w] = 1\n",
    "        #self.inc_word('_LINESPACE')\n",
    "        \n",
    "        for i in range(2,4):\n",
    "            for j in range(len(s)-i+1):\n",
    "                self.inc_word(tuple(s[j:j+i]))\n",
    "\n",
    "    def cond_logprob(self, word, previous):\n",
    "        if (len(previous)>=2 and tuple(previous[-2:] + [word]) in self.model):\n",
    "            return log(self.model[tuple(previous[-2:] + [word])]/self.model[tuple(previous[-2:])],2)\n",
    "        elif (len(previous)>=1 and tuple([previous[-1]] + [word]) in self.model):\n",
    "            return log(self.model[tuple([previous[-1]] + [word])]/self.model[previous[-1]],2)\n",
    "        elif word in self.model:\n",
    "            return log(self.model[word]/len(self.vocabulary),2)\n",
    "        else:\n",
    "            return self.lbackoff\n",
    "\n",
    "    def vocab(self):\n",
    "        return self.vocabulary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, lm, temp = 1.0):\n",
    "        \"\"\"Sampler for a given language model.\n",
    "\n",
    "        Supports the use of temperature, i.e. how peaky we want to treat the\n",
    "        distribution as. Temperature of 1 means no change, temperature <1 means\n",
    "        less randomness (samples high probability words even more), and temp>1\n",
    "        means more randomness (samples low prob words more than otherwise). See\n",
    "        simulated annealing for what this means.\n",
    "        \"\"\"\n",
    "        self.lm = lm\n",
    "        self.rnd = random.Random()\n",
    "        self.temp = temp\n",
    "\n",
    "    def sample_sentence(self, prefix = [], max_length = 20):\n",
    "        \"\"\"Sample a random sentence (list of words) from the language model.\n",
    "\n",
    "        Samples words till either EOS symbol is sampled or max_length is reached.\n",
    "        Does not make any assumptions about the length of the context.\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        sent = prefix\n",
    "        word = self.sample_next(sent, False)\n",
    "        while i <= max_length and word != \"_LINESPACE\":\n",
    "            sent.append(word)\n",
    "            word = self.sample_next(sent)\n",
    "            i += 1\n",
    "        return sent\n",
    "\n",
    "    def sample_next(self, prev, incl_eos = True):\n",
    "        \"\"\"Samples a single word from context.\n",
    "\n",
    "        Can be useful to debug the model, for example if you have a bigram model,\n",
    "        and know the probability of X-Y should be really high, you can run\n",
    "        sample_next([Y]) to see how often X get generated.\n",
    "\n",
    "        incl_eos determines whether the space of words should include EOS or not.\n",
    "        \"\"\"\n",
    "        wps = []\n",
    "        tot = -np.inf # this is the log (total mass)\n",
    "        for w in self.lm.vocab():\n",
    "            if not incl_eos and w == \"_LINESPACE\":\n",
    "                continue\n",
    "                \n",
    "            lp = self.lm.cond_logprob(w, prev)\n",
    "            #wps.append([w, lp/self.temp])\n",
    "            wps.append([w, lp])\n",
    "            tot = np.logaddexp2(lp, tot)\n",
    "        p = self.rnd.random()\n",
    "        word = self.rnd.choice(wps)[0]\n",
    "        s = -np.inf # running mass\n",
    "        for w,lp in wps:\n",
    "            s = np.logaddexp2(s, lp)\n",
    "            if p < pow(2, s-tot):\n",
    "                word = w\n",
    "                break\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textToTokens(text):\n",
    "    \"\"\"Converts input string to a corpus of tokenized sentences.\n",
    "\n",
    "    Assumes that the sentences are divided by newlines (but will ignore empty sentences).\n",
    "    You can use this to try out your own datasets, but is not needed for reading the homework data.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    sents = text.split(\"\\n\")\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit(sents)\n",
    "    tokenizer = count_vect.build_tokenizer()\n",
    "    for s in sents:\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            corpus.append(toks)\n",
    "    return corpus\n",
    "\n",
    "def file_splitter(filename, seed = 0, train_prop = 0.7, dev_prop = 0.15,\n",
    "    test_prop = 0.15):\n",
    "    \"\"\"Splits the lines of a file into 3 output files.\"\"\"\n",
    "    import random\n",
    "    rnd = random.Random(seed)\n",
    "    basename = filename[:-4]\n",
    "    train_file = open(basename + \".train.txt\", \"w\")\n",
    "    test_file = open(basename + \".test.txt\", \"w\")\n",
    "    dev_file = open(basename + \".dev.txt\", \"w\")\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f.readlines():\n",
    "            p = rnd.random()\n",
    "            if p < train_prop:\n",
    "                train_file.write(l)\n",
    "            elif p < train_prop + dev_prop:\n",
    "                dev_file.write(l)\n",
    "            else:\n",
    "                test_file.write(l)\n",
    "    train_file.close()\n",
    "    test_file.close()\n",
    "    dev_file.close()\n",
    "\n",
    "def read_texts(tarfname, dname):\n",
    "    \"\"\"Read the data from the homework data file.\n",
    "\n",
    "    Given the location of the data archive file and the name of the\n",
    "    dataset (one of brown, reuters, or gutenberg), this returns a\n",
    "    data object containing train, test, and dev data. Each is a list\n",
    "    of sentences, where each sentence is a sequence of tokens.\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\", errors = 'replace')\n",
    "    train_mem = tar.getmember(dname + \".train.txt\")\n",
    "    train_txt = unicode(tar.extractfile(train_mem).read(), errors='replace')\n",
    "    test_mem = tar.getmember(dname + \".test.txt\")\n",
    "    test_txt = unicode(tar.extractfile(test_mem).read(), errors='replace')\n",
    "    dev_mem = tar.getmember(dname + \".dev.txt\")\n",
    "    dev_txt = unicode(tar.extractfile(dev_mem).read(), errors='replace')\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit(train_txt.split(\"\\n\"))\n",
    "    tokenizer = count_vect.build_tokenizer()\n",
    "    class Data: pass\n",
    "    data = Data()\n",
    "    data.train = []\n",
    "    for s in train_txt.split(\"\\n\"):\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            data.train.append(toks)\n",
    "    data.test = []\n",
    "    for s in test_txt.split(\"\\n\"):\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            data.test.append(toks)\n",
    "    data.dev = []\n",
    "    for s in dev_txt.split(\"\\n\"):\n",
    "        toks = tokenizer(s)\n",
    "        if len(toks) > 0:\n",
    "            data.dev.append(toks)\n",
    "    print(dname,\" read.\", \"train:\", len(data.train), \"dev:\", len(data.dev), \"test:\", len(data.test))\n",
    "    return data\n",
    "\n",
    "def learn_unigram(data):\n",
    "    \"\"\"Learns a unigram model from data.train.\n",
    "\n",
    "    It also evaluates the model on data.dev and data.test, along with generating\n",
    "    some sample sentences from the model.\n",
    "    \"\"\"\n",
    "    unigram = Unigram()\n",
    "    unigram.fit_corpus(data.train)\n",
    "    print(\"vocab:\", len(unigram.vocab()))\n",
    "    # evaluate on train, test, and dev\n",
    "#     print(\"train:\", unigram.perplexity(data.train))\n",
    "#     print(\"dev  :\", unigram.perplexity(data.dev))\n",
    "#     print(\"test :\", unigram.perplexity(data.test))\n",
    "#     sampler = Sampler(unigram)\n",
    "#     print(\"sample: \", \" \".join(str(x) for x in sampler.sample_sentence([])))\n",
    "#     print(\"sample: \", \" \".join(str(x) for x in sampler.sample_sentence([])))\n",
    "#     print(\"sample: \", \" \".join(str(x) for x in sampler.sample_sentence([])))\n",
    "    return unigram\n",
    "\n",
    "def print_table(table, row_names, col_names, latex_file = None):\n",
    "    \"\"\"Pretty prints the table given the table, and row and col names.\n",
    "\n",
    "    If a latex_file is provided (and tabulate is installed), it also writes a\n",
    "    file containing the LaTeX source of the table (which you can \\input into your report)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from tabulate import tabulate\n",
    "        rows = map(lambda r,t: [r] + t, row_names,table.tolist())\n",
    "        print(tabulate(rows, headers = [\"\"] + col_names))\n",
    "        if latex_file is not None:\n",
    "            latex_str = tabulate(rows, headers = [\"\"] + col_names, tablefmt=\"latex\")\n",
    "            with open(latex_file, 'w') as f:\n",
    "                f.write(latex_str)\n",
    "                f.close()\n",
    "    except ImportError as e:\n",
    "        row_format =\"{:>15} \" * (len(col_names) + 1)\n",
    "        print(row_format.format(\"\", *col_names))\n",
    "        for row_name, row in zip(row_names, table):\n",
    "            print(row_format.format(row_name, *row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "('brown', ' read.', 'train:', 39802, 'dev:', 8437, 'test:', 8533)\n",
      "-----------------------\n",
      "reuters\n",
      "('reuters', ' read.', 'train:', 38183, 'dev:', 8083, 'test:', 8199)\n",
      "-----------------------\n",
      "gutenberg\n",
      "('gutenberg', ' read.', 'train:', 68767, 'dev:', 14667, 'test:', 14861)\n"
     ]
    }
   ],
   "source": [
    "dnames = [\"brown\", \"reuters\", \"gutenberg\"]\n",
    "datas = []\n",
    "# Learn the models for each of the domains, and evaluate it\n",
    "for dname in dnames:\n",
    "    print(\"-----------------------\")\n",
    "    print(dname)\n",
    "    data = read_texts(\"/Users/adityajoshi/UCI/Stats NLP/Language modeling/hw2.gz\", dname)\n",
    "    datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Madam': -2.807354922057604, '_LINESPACE': -2.807354922057604, 'I': -2.807354922057604, 'am': -2.807354922057604, 'only': -2.807354922057604, 'adam': -2.807354922057604, 'your': -2.807354922057604}\n",
      "(0, ':', 'adam only am your only am I')\n",
      "(1, ':', 'your only only only')\n",
      "(2, ':', 'adam your')\n",
      "(3, ':', 'adam adam Madam I I I am')\n",
      "(4, ':', 'Madam I Madam only I adam only Madam am')\n",
      "(5, ':', 'am your am adam I your adam I adam your am am am am am your adam am only Madam')\n",
      "(6, ':', 'I I adam am your only am Madam Madam I')\n",
      "(7, ':', 'your adam adam only I')\n",
      "(8, ':', 'I I am Madam adam')\n",
      "(9, ':', 'I only only')\n"
     ]
    }
   ],
   "source": [
    "unigram = Unigram()\n",
    "corpus = [\n",
    "    [ \"Madam\", \"I\", \"am\", \"your\", \"only\", \"adam\" ]\n",
    "]\n",
    "unigram.fit_corpus(corpus)\n",
    "print(unigram.model)\n",
    "sampler = Sampler(unigram)\n",
    "for i in xrange(10):\n",
    "    print(i, \":\", \" \".join(str(x) for x in sampler.sample_sentence([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "brown\n",
      "('vocab:', 41746)\n",
      "('sample: ', 'is of public of Illinois Stanley the to tragic and her crewcut through he fathers selectivity it to shoot You come')\n",
      "-----------------------\n",
      "reuters\n",
      "('vocab:', 35989)\n",
      "('sample: ', 'each reasonable The pain standards by the by period proposal on said recent White be it dlrs equipment by tax DIVIDEND')\n",
      "-----------------------\n",
      "gutenberg\n",
      "('vocab:', 43736)\n",
      "('sample: ', 'having As Then the 11 here as barn to the think the he glory his bough some time praying at Judah')\n"
     ]
    }
   ],
   "source": [
    "dnames = [\"brown\", \"reuters\", \"gutenberg\"]\n",
    "unigram_models = []\n",
    "i = 0\n",
    "# Learn the models for each of the domains, and evaluate it\n",
    "for dname in dnames:\n",
    "    print(\"-----------------------\")\n",
    "    print(dname)\n",
    "    unigram_model = learn_unigram(datas[i])\n",
    "    unigram_models.append(unigram_model)\n",
    "    sampler = Sampler(unigram_model)\n",
    "    print(\"sample: \", \" \".join(str(x) for x in sampler.sample_sentence([])))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_train = np.zeros((3,))\n",
    "unigram_dev = np.zeros((3,))\n",
    "unigram_test = np.zeros((3,))\n",
    "\n",
    "for i,dname in enumerate(dnames):\n",
    "    unigram_train[i] = unigram_models[i].perplexity(datas[i].train)\n",
    "    unigram_dev[i] = unigram_models[i].perplexity(datas[i].dev)\n",
    "    unigram_test[i] = unigram_models[i].perplexity(datas[i].test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vocab:', 43736)\n",
      "('train:', 981.368830109398)\n",
      "('dev  :', 1012.4294581112321)\n",
      "('test :', 990.082497294399)\n",
      "('sample: ', 'is Doubtfull and their of shall my meanwhile in at 13 upon THAT your isolated which or securing would found in')\n"
     ]
    }
   ],
   "source": [
    "unigram = Unigram()\n",
    "unigram.fit_corpus(data.train)\n",
    "print(\"vocab:\", len(unigram.vocab()))\n",
    "# evaluate on train, test, and dev\n",
    "print(\"train:\", unigram.perplexity(data.train))\n",
    "print(\"dev  :\", unigram.perplexity(data.dev))\n",
    "print(\"test :\", unigram.perplexity(data.test))\n",
    "sampler = Sampler(unigram)\n",
    "print(\"sample: \", \" \".join(str(x) for x in sampler.sample_sentence([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1604.19822047  1501.45588743   990.08249729]\n"
     ]
    }
   ],
   "source": [
    "print(unigram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
